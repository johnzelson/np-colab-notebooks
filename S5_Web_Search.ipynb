{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOE1TenM61I+gSAjhdmkdxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnzelson/np-colab-notebooks/blob/main/S5_Web_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "This notebook launches web searches (using Bing Web Search api) to learn more about an organization when website is not included in IRS Form.  In future, with a website for each organization, scraping can be done to get additional info about each nonprofit.\n",
        "\n",
        "While the IRS Tax docs have lots of info, the web searches can include info is interesting  to someone who wants to learn about their community.  \n",
        "\n",
        "I tried a few ideas, but ended up using Bing free tier (needs a credit card, tho).  It has an api and allows 1000 requests/mo. Fiddled with Duck Duck Go, Wikidata, Wikipeda, but not satisfactory results.  Simply scraping google or bing without registered API seems to be violation of terms.  \n",
        "\n",
        "Roadmap: Once all the websites are populated, can use scrapy or bs4 to dig into nonprofit websites.\n",
        "\n"
      ],
      "metadata": {
        "id": "xDSJXJIG8LZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tech Notes\n",
        "\n",
        "|   In           |   Description |\n",
        "| -------------- | ----------------- |\n",
        "| np_cortland_df    | Local Area df |\n",
        "| irs_latest_df   | Website, if exists, is in Tax Forms |\n",
        "\n",
        "\n",
        "|   Out           |   Description |\n",
        "| -------------- | ----------------- |\n",
        "| web_find_df.csv | Web Search Results  |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RKGSjcTKrPZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "Fo-wYqEkSei2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gYaTJyyavSzl",
        "outputId": "e570fc55-b5c6-4118-c070-60436340d6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRCIX0aCoxi-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 100);\n",
        "\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: create a configuration section -- not implemented\n",
        "\n",
        "# base folder for retrieving raw data\n",
        "data_dir = '/content/drive/My Drive/irs_data/'\n",
        "\n",
        "# folder for writing processed data\n",
        "proc_dir = '/content/drive/My Drive/IRS_processed/'\n"
      ],
      "metadata": {
        "id": "RcpuUJGBj4TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep DFs\n",
        "\n",
        "First, have to get website from listed on IRS Tax Return."
      ],
      "metadata": {
        "id": "AMxFF-yfL3-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from previous processing, load dataframe to\n",
        "\n",
        "#TODO: Again, when to introduce np_local\n",
        "\n",
        "# using np_cortland\n",
        "np_cortland_df = pd.read_csv('/content/drive/My Drive/IRS_processed/np_cortland_p_df.csv')\n",
        "np_cortland_df\n",
        "\n",
        "# need website from IRS Latest\n",
        "IRS_latest_df = pd.read_csv ('/content/drive/My Drive/IRS_processed/irs_latest_df.csv')\n",
        "IRS_latest_df\n",
        "\n",
        "# create np_local_df (used that name previously in code) but it's just a temp name\n",
        "# for this notebook\n",
        "#TODO: reviewing naming strategies for each step\n",
        "np_local_df = pd.merge(\n",
        "    np_cortland_df,\n",
        "    IRS_latest_df[['EIN', 'WebsiteAddressTxt']],  # Select desired columns\n",
        "    on='EIN',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "filt =  (np_local_df['WebsiteAddressTxt'] == \"tag_not_found\") |  np_local_df['WebsiteAddressTxt'].isnull()\n",
        "np_local_df[filt].shape # 117\n",
        "\n"
      ],
      "metadata": {
        "id": "O2zyaHpfx-Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9febd1b-d147-4b2b-ede4-d974d999f531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(117, 68)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lookup with Bing"
      ],
      "metadata": {
        "id": "x9_gFh-437fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# kickoff search\n",
        "# --------------------\n",
        "# iterate np_local_df\n",
        "# filter where website has \"tag_not_found_ or is null\n",
        "# save results with p_org_id and ein in web_srch_df\n",
        "\n",
        "# null website:  no tax return was found for nonprofit\n",
        "# tag_not_found:  a tax return was processed, but didn't have a website\n",
        "\n",
        "import requests\n",
        "import pprint\n",
        "\n",
        "from google.colab import userdata\n",
        "subscription_key = userdata.get('bing_key')\n",
        "\n",
        "search_url = \"https://api.bing.microsoft.com/v7.0/search\"\n",
        "\n",
        "# list of dicts for each search\n",
        "web_find_list = []\n",
        "each_find = {}\n",
        "\n",
        "\n",
        "# filter dataframe cortland_df for website is null\n",
        "# filt =  cortland_geotax_df['WebsiteAddressTxt'].isnull()\n",
        "\n",
        "# should have added tag_not_found to filter\n",
        "\n",
        "filt =  (np_local_df['WebsiteAddressTxt'] == \"tag_not_found\") |  np_local_df['WebsiteAddressTxt'].isnull()\n",
        "\n",
        "\n",
        "#for index, row in cortland_geotax_df[filt].head(3).iterrows():\n",
        "for index, row in np_local_df[filt].iterrows():\n",
        "\n",
        "  each_find = {}\n",
        "  p_org_id = row['p_org_id']\n",
        "  ein = row['EIN']\n",
        "  np_name= row['NAME']\n",
        "  city = row['CITY']\n",
        "  state = row['STATE']\n",
        "\n",
        "  # go do search\n",
        "  print (f\"searching {np_name} ({ein})...\")\n",
        "  each_find = bing_lookup(ein, np_name, city, state)\n",
        "  each_find['p_org_id'] = p_org_id\n",
        "  each_find['ein'] = ein\n",
        "\n",
        "  web_find_list.append(each_find)\n",
        "\n",
        "#web_find_tnf_df = pd.DataFrame.from_dict(web_find_list)\n",
        "web_srch_df = pd.DataFrame.from_dict(web_find_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yglMajXcSDjD",
        "outputId": "583e563a-2c4b-4f7d-8f4f-b4a2c002fbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching CORTLANDVILLE FIRE DEPARTMENT INCORPORATED (115227037)...\n",
            "searching 1890 HOUSE MUSEUM AND CENTER FOR THE ARTS (132951986)...\n",
            "searching CORTLAND RURAL CEMETERY (150279170)...\n",
            "searching BENEVOLENT & PROTECTIVE ORDER OF ELKS OF THE USA (150298495)...\n",
            "searching AMERICAN LEGION (150610966)...\n",
            "searching RALPH WILKINS FOUNDATION (161188525)...\n",
            "searching THE GREAT CORTLAND PUMPKINFEST INC (161506254)...\n",
            "searching CIVIL SERVICE EMPLOYEES ASSOCIATION (161513551)...\n",
            "searching YAMAN FOUNDATION (161571985)...\n",
            "searching THE KINGS DAUGHTERS HOME FOR CHILDREN (166054829)...\n",
            "searching DISABLED VETS OF CORTLAND COUNTY (222315953)...\n",
            "searching FRIENDS OF THE CORTLAND COUNTY CHILD ADVOCACY CENTER INC (462277341)...\n",
            "searching DAN AND ROSE MCNEIL FOUNDATION INC (833255297)...\n",
            "searching CORTLAND REUSE INC (853825876)...\n",
            "searching LITTLE TAGS FOUNDATION INC (870850886)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def to lookup np org\n",
        "# return website, snippet, and store entire search results in dataframe\n",
        "  # https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/reference/query-parameters\n",
        "\n",
        "def bing_lookup(ein, np_name, city, state):\n",
        "\n",
        "  each_bing_find = {}\n",
        "\n",
        "\n",
        "  # test in browser with EIN\n",
        "  search_term = np_name + \" in \" + city + \", \" + state\n",
        "\n",
        "  headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
        "  params = {\"q\": search_term, \"textDecorations\": True, \"textFormat\": \"HTML\"}\n",
        "  #params = {\"q\": search_term}\n",
        "  response = requests.get(search_url, headers=headers, params=params)\n",
        "  response.raise_for_status()\n",
        "  search_results = response.json()\n",
        "  #pprint.pprint(search_results)\n",
        "\n",
        "  # just getting first web page.\n",
        "  # TODO: add some intelligence, tho' can do it later since saving full\n",
        "  url = search_results['webPages']['value'][0]['url']\n",
        "  found_name = search_results['webPages']['value'][0]['name']\n",
        "  snippet = search_results['webPages']['value'][0]['snippet']\n",
        "  full_result = response.text\n",
        "\n",
        "  each_bing_find['url'] = url\n",
        "  each_bing_find['found_name'] = found_name\n",
        "  each_bing_find['snippet'] = snippet\n",
        "  each_bing_find['full_result'] = full_result\n",
        "\n",
        "  return each_bing_find\n",
        "\n"
      ],
      "metadata": {
        "id": "7U9VO5WC3MK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save results of web search into web_find_df\n",
        "fn = proc_dir + 'web_find_df.csv'\n",
        "#web_srch_df.to_csv('/content/drive/My Drive/IRS_geocode/web_find_df.csv')\n",
        "web_srch_df.to_csv(fn, index=False)\n",
        "print (f\"saved {fn}\"\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MYLU8fWF_CHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}